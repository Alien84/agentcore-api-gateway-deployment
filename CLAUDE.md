# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is an Amazon Bedrock AgentCore crash course repository demonstrating how to build AI agents with increasing complexity. The project contains three progressive examples showing the evolution from a basic LangGraph agent to a fully managed AgentCore agent with memory capabilities.

## Commands

### Environment Setup
```bash
# Install dependencies using uv (recommended)
uv sync

# Activate virtual environment
source .venv/bin/activate  # On macOS/Linux
```

### Running Examples

**Example 1: Basic LangGraph Agent**
```bash
python 00_langgraph_agent.py
```

**Example 2: AgentCore Runtime Agent**
```bash
# Configure the agent (generates .bedrock_agentcore.yaml)
agentcore configure -e 01_agentcore_runtime.py

# Launch the agent runtime
agentcore launch --env GROQ_API_KEY=your_groq_api_key_here

# Invoke the deployed agent
agentcore invoke '{"prompt": "Explain roaming activation"}'
```

**Example 3: AgentCore with Memory**
```bash
# Configure the agent with memory settings
agentcore configure -e 02_agentcore_memory.py

# Launch the agent runtime
agentcore launch --env GROQ_API_KEY=your_groq_api_key_here

# Invoke with session tracking
agentcore invoke '{"prompt": "Remember my preference and answer my question"}'
```

## Architecture

### Three-Tiered Learning Progression

The codebase is structured as three progressively complex implementations:

1. **00_langgraph_agent.py** - Standalone LangGraph agent
   - Demonstrates basic agent creation with LangChain
   - Uses FAISS vector store for FAQ retrieval
   - No AgentCore integration
   - Entry point: Direct script execution

2. **01_agentcore_runtime.py** - AgentCore runtime integration
   - Same agent logic as Example 1
   - Adds `BedrockAgentCoreApp()` wrapper
   - Introduces `@app.entrypoint` decorator for AgentCore deployment
   - Payload structure: `{"prompt": "user query"}`
   - Response structure: `{"result": "agent response"}`

3. **02_agentcore_memory.py** - Full memory management
   - Extends Example 2 with memory capabilities
   - Uses `AgentCoreMemorySaver` for checkpointing conversation state
   - Uses `AgentCoreMemoryStore` for long-term memory storage
   - Implements `AgentMiddleware` with pre/post-model hooks
   - Tracks `actor_id` and `thread_id` for session management
   - Memory namespace pattern: `(actor_id, thread_id)` for conversations, `("preferences", actor_id)` for user preferences

### Common Architecture Patterns

All three examples share:

**Vector Store Setup**
- Load FAQ data from `lauki_qna.csv`
- Use HuggingFace embeddings (`sentence-transformers/all-MiniLM-L6-v2`)
- Create FAISS vector store with 500-character chunks
- RecursiveCharacterTextSplitter with no overlap

**Tool Definitions**
- `search_faq(query)` - Returns top 3 relevant FAQ entries
- `search_detailed_faq(query, num_results=5)` - Returns more comprehensive results
- `reformulate_query(original_query, focus_aspect)` - Searches with reformulated focus

**LLM Configuration**
- Uses Groq LLM provider with `openai/gpt-oss-20b` model
- Temperature set to 0 for deterministic responses
- API key loaded from environment variable `GROQ_API_KEY`

### Memory Architecture (Example 3)

The memory system uses a two-layer approach:

1. **Short-term Memory (Checkpointer)**
   - `AgentCoreMemorySaver(memory_id=MEMORY_ID)` maintains conversation state
   - Automatically persists message history within a session
   - Keyed by `thread_id`

2. **Long-term Memory (Store)**
   - `AgentCoreMemoryStore(memory_id=MEMORY_ID)` enables semantic search across sessions
   - Pre-model hook: Saves user messages and retrieves relevant memories
   - Post-model hook: Saves AI responses for future retrieval
   - Namespace isolation: Different actors/threads don't cross-contaminate

**MemoryMiddleware Hooks**
- `pre_model_hook()`: Executes before LLM invocation to inject retrieved memories into context
- `post_model_hook()`: Executes after LLM response to persist AI messages

## Configuration Files

### .bedrock_agentcore.yaml
Generated by `agentcore configure` command. Contains:
- Agent deployment configuration (container, runtime, platform)
- AWS resource settings (execution role, ECR repository, region)
- Memory configuration (mode, memory_id, event expiry)
- Network and observability settings

Key fields to modify:
- `entrypoint`: Path to your agent file
- `region`: AWS region for deployment
- `memory.mode`: Set to enable/disable memory features
- `memory.memory_id`: Reference to AgentCore Memory instance

### pyproject.toml
Defines project dependencies:
- `bedrock-agentcore`: Core AgentCore SDK
- `bedrock-agentcore-starter-toolkit`: CLI tools
- `langchain-*`: LangChain ecosystem packages
- `langgraph`: Agent graph framework
- `langgraph-checkpoint-aws`: Memory checkpointing
- `faiss-cpu`: Vector similarity search
- `sentence-transformers`: Embedding models

### .env
Required environment variables:
- `GROQ_API_KEY`: API key for Groq LLM service
- `HF_API_KEY`: HuggingFace API key (optional, for some embeddings)

## AgentCore Runtime Patterns

### Entrypoint Function Signature
```python
@app.entrypoint
def agent_invocation(payload, context):
    # payload: dict containing user input
    # context: AgentCore runtime context
    # Returns: dict with agent response
```

### Memory-Enabled Invocation Pattern
```python
config = {
    "configurable": {
        "thread_id": thread_id,  # Session identifier
        "actor_id": actor_id     # User identifier
    }
}
result = agent.invoke({"messages": [("human", query)]}, config=config)
```

### Payload Conventions
- Input: `{"prompt": "user query", "actor_id": "user-123", "thread_id": "session-456"}`
- Output: `{"result": "agent response", "actor_id": "user-123", "thread_id": "session-456"}`

## Important Notes

- The MEMORY_ID in `02_agentcore_memory.py:31` is hardcoded and specific to a deployed AgentCore Memory instance
- FAISS stores are created in-memory at runtime; they are not persisted between runs
- The `lauki_qna.csv` dataset contains FAQ data about a telecom/mobile service (Lauki)
- All examples use the same system prompt structure with minor variations for memory-enabled agents
- AWS credentials must be configured via AWS CLI before deploying agents